---
title: "Statistics in R - Team Project"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simul}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Simulation Study
## Introduction
In the following the four main methods of this package are studied experimentally. The four techniques include decision and regression trees generated by the CART algorithm, pruned CART trees, the CART algorithm augmented by the bootstrap aggregating procedure (bagging) and lastly Random Forests. After looking at the methods themselves and some of their hyperparameters in the first section, the methods are compared qualitatively as well as quantitatively in the second section. Most of the results present some form of regression problem. Note that all the results very well are reproducible in the classification case. See the shiny-app for reference. In the third section the methods are applied to a small real data example: The Iris Dataset. The simulation study is closed ba a short quantitative analysis of the performance optimizing method of using quantiles in the process of finding optimal splits instead of single samples only.

## A Deeper Look Into the Methods
### CART Regression Trees
In this section the induced predictor of the CART generated regression tree is analysed for a one-dimensional case. The data is generated by the underlying generative function $y = sin(2*\pi*x) + \epsilon$ where $\epsilon \sim N(0, \sigma)$ and $\sigma$ is chosen to be equal to $0.25$. The CART algorithm is used to fit a regression tree to the data $x$, predicting the value $y$. The most important hyperparameter, the depth of the tree, is varied. We randomly draw $100$ samples from the uniform distribution $unif(0, 1)$ and plot the resulting predictor for different depths.

```{r setup}
# library(obstgarten)
suppressMessages(devtools::load_all())
suppressMessages(library(obstgarten))
```

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(simul_plot_greedy())
```

Immediately, one can see that with increasing depth the model's complexity grows and the tree induced predictor (blue) overfits the datapoints. When the depth is too small as in the case in the top left corner the model underfits the data. Out of these four examples depth $= 5$ seems to be the best choice. The MSE of the prediction suggests the same. It gets apparent that the depth of the tree is an essential model hyperparameter. To further investigate this the fitting process is repreated $400$ times and for every datapoint $x_i$ the mean prediction and standard deviation is computed over the $400$ repetitions. The predictor's means and standard deviations of trees of different depth are compared.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
load("data/bv_greedy")
suppressMessages(bv_plot(bv_data, plot_title = "CART Bias-Variance Tradeoff"))
```

In terms of the Bias-Variance decomposition of the prediction error a model is called optimal if it has minimum Bias and minimum variance. Thus, the closer the predictor mean is to the true underlying generative function, the lower the Bias. Furthermore, the narrower the confidence bands of the predictors the smaller the variance of the model. As a result, it can be concluded that all of the four tested trees except for the tree with depth $= 2$ show a very small bias and fit to the true function very well. Additionally, it can be observed that the model variance increases with model complexity as expected. While the Bias does barely change for depths bigger than five, the variance does increase. Unsurprisingly, it can be concluded that the CART generated tree with depth $5$ is the model which experimentally shows the lowest combined Bias and variance and consequently prediction error. This topic could be analysed in utter detail as the Bias-variance tradeoff is a central topic of statistical learning and going into more detail here would go beyond the scope of this study.

### Pruning 
Pruning is a regularization technique which is used to cut back fully grown CART generated rergession and decision trees. The idea behind it is to impose an additional loss which scales with the depth of the tree. In the following the same dataset as in the section before is used and the cost-complexity paramter lambda which controls the amount of regularization is varied. 

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
# suppressMessages(simul_plot_pruning())
```

The Bias-Variance tradeoff is displayed qualitatively in the next plot. 

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
#load("data/bv_pruning")
#suppressMessages(bv_plot(bv_data, plot_title = "Pruning Bias-Variance Tradeoff", bagging = FALSE, pruning=TRUE))
```

### Bootstrap Aggregating (Bagging)
The bootstrap aggregating (Bagging) procedure is an ensemble method from statistical machine learning. It can reduce the model's variance, enhance the accuracy and has a regularization effect. The Bagging procedure adds randomness to the model by averaging (regression) or majority-voting (classification) the prediction for samples $x_i$ over the different models that were fitted on the bootstrap samples. The bootstrap samples are created by randomly drawing n samples from the dataset of size n with replacement. The number of bootstrap samples B is a hyperparameter of the model. In the following the same generative function is used as in the two sections before. Now the trees are generated by the well-known bootstrap aggregating procedure in combination with the CART algorithm. The number of bootstrap samples $B$ is varied in the following and prediction quality is compared. The tree depth is $5$.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(simul_plot_bagging())
```

The predictor in the top left corner is equivalent to a single CART generated regression tree. One can see that the prediction is already pretty decent but the more bootstrap samples are added to the Bagging procedure the smoother the prediction gets. The MSE suggests the same. Looking at the Bias-Variance tradeoff again the results should look very sound.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
load("data/bv_bagging")
suppressMessages(bv_plot(bv_data, plot_title = "Bagging Bias-Variance Tradeoff", bagging = TRUE))
```

All predictor means already are pretty close to the true function suggesting a small Bias. A significant decrease in variance can be observed between the case $B = 1$ and $B = 5$. A further decrease would be expected for a larger number of bootstrap samples. Nevertheless, this cannot be confirmed in this case. This most probably is due to the fact that on one hand the dataset is very small and on the other hand the underlying generative function is very simple. For more complex generative functions we can observe that with a larger number of bootstrap samples model variance is strongly reduced. 

### Random Forests

Random Forests introduce more randomness into the fitting process by not only using Bagging but also utilizing random dimensionality subsets of the data. For every fitted random tree, a number of $m$ feature dimensions are randomly chosen and the other $d-m$ dimensions are discarded for this fitting process. In the end the mean is taken over all random trees in the regression case or a majority vote is employed in the classification case. Random Forests are a powerful ensemble method from the area of machine learning. They can for example with only a few hundred datapoints predict the shape of a two dimensional function. In the following plot a Random Forest is employed to fit to the function $f(x, y) = \frac{sin(\sqrt{x^2+y^2})}{\sqrt{x^2+y^2}}$ with only $1000$ samples genereated by this function with added noise $\epsilon \sim N(0, \Sigma)$. A contour plot is utilized to visualize the prediction. The contour lines of the true values are added on top of the plot. 

```{r, warning=FALSE, fig.width=7.25, fig.height=6}
suppressMessages(pred_plot_sine2D(n=1000, B=10L, depth=5, sd=0.05, k=10, random_forest=TRUE))
```

In the following the influence of the random dimensionality subset parameter $m$ on the prediction is investigated. $1000$ samples $x$ are drawn from a 4-dimensional multivariate gaussian with zero mean and identity covariance. Noise $\epsilon \sim N(0, \Sigma)$ is added to the resulting observations $y$. $100$ bootstrap samples are used, $\sigma$ is set to $0.25$.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(plot_3D_compare_m("data/compare_RF_m_d4", margin=0.5, render = FALSE))
```
The first two dimensions and the second two dimension are plotted against each other and the resulting probability distributions are compared to the true distributions. As one can see, the model does not manage to predict the true distribution well enough if the random trees are fitted on $m = 1$ dimension only. The random forest prediction for $m=2$ and $m=3$ are significantly better and even though $m=3$ seems to have the optimal prediction $m=2$ is good enough to settle on considering computation time. 

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(plot_3D_compare_m_DIFF("data/compare_RF_m_d4", margin=0.2, render = FALSE))
```
Above, instead of the predicted distributions, the squared differences between every single sample $x_i$ and the true distribution are displayed. Here, it gets even more apparent that the model with $m = 3$ outperforms both the models $m = 1$ and even $m = 2$. Nevertheless, the difference is rather marginal.

In higher dimensions fitting trees to random dimensional subsets of the features can not only improve prediction quality but also reduce computational cost by a large margin. Additionally, the prediction error of the model was approximated by repeating the fitting process for all Random Forests $100$ times. The same dataset, multivariate gaussian and noise term as above was used. The quantitative results suggest the same as the plots above. The higher the model hyperparameter $m$ the lower the overall prediction error. Nonetheless, computational cost increases with $m$. See in the table below:

|                  | $m = 1$ | $m = 2$ | $m = 3$ |
|:-----------------:|:------:|:-----:|:---------: |
| Prediction Error |   3.8353  |  0.3036  |    0.0756   |

## Comparing the Methods
In this section the prediction performance of the methods are compared. To do so, the same multidimensional dataset as in section two is employed and the results are evaluated qualitatively as well as quantitavely in form of the prediction error.

When looking at the figures below one the fact gets apparent that the Random Forest again models the true probability distribution the most accurate. In contrast, the CART predictor shows some distinct vertical boundaries on regression values where the values suddenly jump. The Bagging algorithm does better in that regard. Nevertheless, all methods fail to capture the more extreme values around the mean of the gaussian. This is probably due to the inherent problem, that if one projects the other dimensions onto this one dimension and fits a regression tree on it, the majority of samples will be close to zero as they are far away from the gaussian mean in one or more of the other dimensions. Thus, the predictor of the trees is drawn downwards. The higher the dimensionality of the multivariate gaussia the bigger this inherent problem becomes. 

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(plot_3D_compare("data/compare_methods_4d_wo_pruning", margin=1., render = FALSE))
```
This time, the actual predicted densities seem to be more expressive than the squared differences in the next figure. 

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(plot_3D_compare_DIFF("data/compare_methods_4d_wo_pruning", margin=0.35, render = FALSE))
```

As can be seen in the quantitative analysis below where the prediction error was approximated by repeating the calculation $100$ times one can see that the results from above are supported by the prediction error. The Random Forest performs best, followed by the Bagging algorithm and lastly the CART generated trees.  

|                  | CART | Pruning | Bagging | Random Forest |
|:-----------------:|:------:|:-----:|:---------:|:------:|
| Prediction Error |   1.44  |  ./.  |    1.35   |    1.27  |

## A Small Real Data Application: The Iris Classification Problem
In the following part of the study the methods are applied to real data. The Iris Dataset is used here. Which is a 4-dimensional supervised classification problem. The 4-dimensional feature vectors describe the measures of three different flowers including sepal length/width and petal length/width. The task is to learn to correctly classify the feature vectors as either the type "setosa", "versicolor" or "virginica". The distributions of the features is displayed in the following plot.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
visualize_iris_feature_distr()
```

As one can see the flower of the type "setosa" is rather well seperated and distinguishable by the features. It should be more difficult for the model to distinguish between the versicolor and virginica. 

To train and evaluate the model on this dataset it was split randomly into training (0.8) and test set (0.2) and the models were trained on the training set and prediction quality was tested on the test set. All four models were tested and the results were convincing. One has to keep in mind that this dataset is rather small with $150$ samples, $50$ of each class. From this perspective this is an easy classification problem. Nevertheless, it can suffice to test fundamental functionality of the devoloped methods. 

A Random Forest with $m = 1$, depth $5$ and $10$ bootstrap samples is perfectly able to classify the flowers to a high accuracy. See below:

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(pred_plot_iris_class(depth=5, B=10L))
```
Sepal length is on the x-axis while sepal width is on the y axis.

When comparing the methods quantitavely one can see that the quality of the predictions of the model are all very high and that feature densities can easily be seperated even by the simplest model. Additionaly, in this case the Random Forest performs worse probably due to the additional randomness and regularization the random dimensionality subsets bring. All the more, it really depends which samples are in the training set and which are the test set.

|                  | CART | Pruning | Bagging | Random Forest |
|:-----------------:|:------:|:-----:|:---------:|:------:|
| Accuracy |   1.000  | ./.   |    1.000   |    0.967  |

Therefore, the process is repeated $100$ times and average accuracy is approximated. The result is as expected: All the models can fit the data very well. 

|                  | CART | Pruning | Bagging | Random Forest |
|:-----------------:|:------:|:-----:|:---------:|:------:|
| Accuracy |   0.937  | ./.   |    0.940   |    0.933  |

## Performance Analysis: The Quantile Approach
Again a Random Forest is employed to fit to the function $f(x, y) = \frac{sin(\sqrt{x^2+y^2})}{\sqrt{x^2+y^2}}$ with only $1000$ samples generated by this function with added noise $\epsilon \sim N(0, \Sigma)$. The number of bootstrap samples was chosen to be $B = 10$ (for a higher number of bootstrap samples the improvement would be even higher). Nevertheless, the performance should be hit harder in cases where Bagging is not utilized and predictions are not averaged over the bootstrap samples. Thus, those two methods do combine extraordinary well. The depth of the trees is $5$. The experiment is repeated $100$ times to estimate the prediction error and the average duration of fitting a Random Forest to one generated dataset. The minimum number of samples in a quantile was set to $100$ and  The results are pretty convincing: 

|                  | Without Qantiles | With Quantiles |
|:-----------------:|:------:|:-----:|
| Prediction Error |   0.0118  |  0.0114  |
| Fitting Time [sec] | 12.98  | 5.71 |

While the approximated prediction error does barely change, the time it takes to fit a tree to the data is reduced by more than $6$ seconds.
