---
title: "Obstgarten Overview"
author: "Obstgarten team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Obstgarten Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Contents
Introduction

1. Datenstrukturen
    1. obst.R 
1. Algorithmen
    1. greedy.R 
    1. bagging.R
    1. pruning.R
1. Simulationsstudie
    1. bv.R
    1. data.R
    1. pred.R
    1. simul.R
    1. plot3d.R
1. Shiny
    1. shiny.R
1. Performance considerations

## Introduction

Estimation algorithms using trees are a popular class of simple regression and classification estimators, due to their simple tractable structure and comparatively fast evaluation.
This package aims to provide the most popular of these estimators in an easy-to-use implementation.

The given functions are sufficiently fast and versatile to enable the user to explore their behaviour and to experience the effect of their respective hyperparameters. A multitude of visualization methods and an easy to use shiny app leave the user with little to ask for in their quest of evaluating tree-based regression and classification estimators.

### Trees

Trees are implemented in this package using the following classes:
- `Gabel`, an `R6` class representing a recursive tree data structure with pointers to children and parent nodes. Necessary attributes for use wit CART algorithms are included;
- `Baum`, an `R6` class saving a list of `Gabel` nodes for easy introspection and validation.

As these classes use `R6`, data is stored in environments and reference semantics are used. Trees are created with `Baum$new()` and built up with `Baum$append()`, latter providing a labeling of nodes in *BFS* order.

```{r}
# create new tree and 2 leaves
T1 <- Baum$new()
G1 <- T1$root
G2 <- Gabel$new()
G3 <- Gabel$new()

# populate attributes with random data
G1$s <- runif(1)
G1$j <- sample(10, 1)
G2$y <- runif(1)
G2$y <- runif(1)

# append nodes to root
T1$append(G1, G2, G3)

# view tree as list of lists
str(as.list(T1$root))

# view nodes of tree as list
T1$nodes
```

Data structure (recursive) representing regression and classification trees (CART). Edges are bi-directional (inspired from `data.tree::Node`). Data is stored in environments (`R6`, reference semantics).

## 2 Algorithms

### 2.1 greedy.R 
 
Generating a single CART using the greedy algorithm, optional sampling. Beiing used in random forests.

#### 2.1.1 bagging.R 
 
Generating random forests, with or without sampling.
The bootstrap aggregating (Bagging) procedure is an ensemble method from statistical machine learning. It can reduce the model's variance, enhance the accuracy and has a regularization effect. The Bagging procedure adds randomness to the model by averaging (regression) or majority-voting (classification) the prediction for samples $x_i$ over the different models that were fitted on the bootstrap samples. The bootstrap samples are created by randomly drawing n samples from the dataset of size n with replacement. The number of bootstrap samples B is a hyperparameter of the model.

Richter, Definition 6.25, 6.26, 6.28


#### 2.1.2 pruning.R

Cost-complexity pruning of a greedily generated CART, according to a given complexity weight. 
Pruning is a regularization technique which presents a way to decrease computational complexity of empirical estimators as well as simple generalization for overfitting-prone approaches. The cost-complexity parameter lambda controls the amount of regularization.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(simul_plot_pruning())
```
```{r, warning=FALSE, fig.width=7.25, fig.height=5}
load("data/bv_pruning")
suppressMessages(bv_plot(bv_data, plot_title = "Pruning Bias-Variance Tradeoff", bagging = FALSE, pruning=TRUE))
```

## 3 Simulationsstudie

### 3.1 bv.R

Bias-variance

### 3.2 data.R 

Generating random data conforming to diverse demands.

### 3.3 pred.R 

Prediction/MSE of CARTs

### 3.4 simul.R

Plots and simulation study

### 3.5 plot3d.R 

3D plotting methods

## 4 Shiny

### 4.1 shiny.R

Interactive shiny app, sporting all the algorithms above.

## 5 Performance considerations


In order to improve the performance of the greedy-algorithm (which is beeing called repeatedly in bagging, pruning an random forests), there are the following approaches:

* Dynamic programming (*top-down*): memorization of intermediate results
* Dynamic programming (*bottum-up*): build up the tree from below
* Parallelization: split the algorithms at the highest sensible layer into parallel threads
* Quantiles: reduce the amount of data-points for the minimization-problem 

**Dynamic programming** proved to be problematic. The package *memoize* showed in preliminary tests to lead to enormous (up to 700%) losses in performance.

**Parallelization** easy to implement, since the costly loops are generally over the whole tree and can be simply split. The package `parallel` provides derivatives of the `apply`-method. But this works easily only on Linux-systems and has a huge overhead, significantly diminishing the craved results.

* On *Windows*-machines, there are no child-processes as under Linux, necessitating the usage of a cluster-scheme, which leads to difficulties in actually using functions and variables defined elsewhere, especially in other packages.
* On *Linux*-machines there is `fork()` for parallel processes, which inherit all data obliviating the need for explicit exportation. But GUI-systems (commonly found on workstations) prohibit the usage of these, since they interfere with their inner workings (leading to frequent but intractabel errors)

In the end, those approaches did not lead to noticable gains, mostly the opposite.

**Idea for quantiles:** in order to reduce the complexity-increase associated with the number of observations, one can use quantiles. This means basic pooling of observations into classes which then constitute the elements of further computation. This allows for a flexible reduction of the data-set to manageable sizes. 

since most other algorithms base on cart-greedy, we implemented this there, leading to great reductions in computational cost while maintaining accuracy as needed.

`cart_greedy` was extended with the following parameters:

* `q_threshold`: minimal amount of data points to use quantiles.
* `q_pct`: amount of probabilities for `quantile()`, in pct. of the data set size.
* `type`: `quantile(..., type)`, type of quantile algorithm

Using the `quantile()`-function needs heeding of the fact, that it is only applicable to 1-dimensional vectors. For higher dimensions, one can use for example `sapply`. 



