---
title: "Obstgarten Overview"
author: "Obstgarten team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Obstgarten Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Contents
Introduction

1. Datenstrukturen
    1. obst.R 
1. Algorithmen
    1. greedy.R 
    1. bagging.R
    1. pruning.R
1. Simulationsstudie
    1. bv.R
    1. data.R
    1. pred.R
    1. simul.R
    1. plot3d.R
1. Shiny
    1. shiny.R
1. Performance considerations

## Introduction

Estimation algorithms using trees are a popular class of simple regression and classification estimators, due to their simple tractable structure and comparatively fast evaluation.
This package aims to provide the most popular of these estimators in an easy-to-use implementation.

The given functions are sufficiently fast and versatile to enable the user to explore their behaviour and to experience the effect of their respective hyperparameters. A multitude of visualization methods and an easy to use shiny app leave the user with little to ask for in their quest of evaluating tree-based regression and classification estimators.

## 1 Datenstrukturen

### 1.1 obst.R 

Data structure (recursive) representing regression and classification trees (CART). Edges are bi-directional (inspired from `data.tree::Node`). Data is stored in environments (`R6`, reference semantics).

## Attributes
The following attributes are included for a node v:

- `childL`, `childR` for the left and right child node, respectively;
- `parent`, for the parent node;
- `label`, the unique labeling of the node;
- `depth`, the node depth (the length of the chain `v_0`, `v_1` ... `v`);
- `points`, the intersection of the training data and partition A(v);
  + **TODO**: Storing (subsets of) the training data in nodes is an inefficient use of space. An alternative (for high dimensional data specifically) is to only store the indices of the partition of the training data in every node.
- `s`, the split point (for an inner node);
- `j`, the split index (for an inner node);
- `y`, value (for a leaf; numeric for a regression tree, integer for a classification tree)

## Methods
The following methods are included:

- `isObst()`: determines if the node is a leaf (`TRUE`) or not (`FALSE`).
- `print()`: print common attributes (all except `points`); attributes set to `NULL` are not printed.



A helper class to build a CART based on `Gabel` nodes. Nodes are accessible through a list for easy introspection with `apply`. Nodes are labeled in BFS order.

As `Gabel`, data is stored in environments (`R6`, reference semantics).

## Attributes

- `nodes`: list of `Gabel` nodes. It is assumed that the label of a node in `Baum` matches its index in this list.
- `root`: the root node of tree.

## Methods

- `initialize()`: create a new `Baum` object with a single root node (label `1L`, depth `0L`)
- `obstkorb()`: returns a logical vector, with `TRUE` for leaves and `FALSE` otherwise.
- `append(label, Child1, Child2)`: appends the argument nodes `Child1`, `Child2` to the node in the tree with `label` (the parent). Attributes for the parent, `Child1` and `Child2` are updated accordingly (`parent`, `depth`, `childL`, `childR`). Labels are assigned in a breadth-first-search manner. 
*Note*: When only using this method to construct the tree, no cycles are possible.
- `partition(node, d)`: Creates a partition of d-dimensional space based on the split points and indices in the tree nodes. Returns a matrix with the split indices as columns, and the split points as rows.
- `predict(x)`: Predict the value of a datum `x`, based on the split points and indices in the tree nodes. Complexity: `O(log n)` in the amount of nodes.
- `validate()`: Ensures that the following criteria are fulfilled:
*Note*: Cycles are not checked in this method.
  * all node labels are unique (integers);
  * all leaves have `j`, `s` unset (`NA`) and `y` set;
  * all inner nodes have `j`, `s` set (`integer` and `numeric`, respectively) and `y` unset (`NA`)
  * ensure all leaves have at least a single data point.
- `depth()`: Return the depth of the tree (the maximum of the depth of leaves in the tree)

## Notes
In implementing this class, we found the following difficulties:

- Decide on the most suitable data structure to represent binary trees in general, and CARTs specifically.
- Create an interface that is reasonably easy to use for package users.
- Find a suitable scheme for labeling tree nodes (`append()`, `self$nodes`)
- Find a suitable way to represent the subdivision of d-dimensional space, in particular for `d > 1`



## 2 Algorithmen

### 2.1 greedy.R 
 
Generating a single CART using the greedy algorithm, optional sampling. Beiing used in random forests.

#### 2.1.1 bagging.R 
 
Generating random forests, with or without sampling.
The bootstrap aggregating (Bagging) procedure is an ensemble method from statistical machine learning. It can reduce the model's variance, enhance the accuracy and has a regularization effect. The Bagging procedure adds randomness to the model by averaging (regression) or majority-voting (classification) the prediction for samples $x_i$ over the different models that were fitted on the bootstrap samples. The bootstrap samples are created by randomly drawing n samples from the dataset of size n with replacement. The number of bootstrap samples B is a hyperparameter of the model.

Richter, Definition 6.25, 6.26, 6.28


#### 2.1.2 pruning.R

Cost-complexity pruning of a greedily generated CART, according to a given complexity weight. 
Richter, Definition 6.18
Pruning presents a way to decrease computational complexity of empirical estimators as well as simple generalization for overfitting-prone approaches


## 3 Simulationsstudie

### 3.1 bv.R

Bias-variance

### 3.2 data.R 

Generating random data conforming to diverse demands.

### 3.3 pred.R 

Prediction/MSE of CARTs

### 3.4 simul.R

lots and simulation study

### 3.5 plot3d.R 

3D plotting methods

## 4 Shiny

### 4.1 shiny.R

Interactive shiny app, sporting all the algorithms above.

## 5 Performance considerations


In order to improve the performance of the greedy-algorithm (which is beeing called repeatedly in bagging, pruning an random forests), there are the following approaches:

* Dynamic programming (*top-down*): memorization of intermediate results
* Dynamic programming (*bottum-up*): build up the tree from below
* Parallelization: split the algorithms at the highest sensible layer into parallel threads
* Quantiles: reduce the amount of data-points for the minimization-problem 

**Dynamic programming** proved to be problematic. The package *memoize* showed in preliminary tests to lead to enormous (up to 700%) losses in performance.

**Parallelization** easy to implement, since the costly loops are generally over the whole tree and can be simply split. The package `parallel` provides derivatives of the `apply`-method. But this works easily only on Linux-systems and has a huge overhead, significantly diminishing the craved results.

* On *Windows*-machines, there are no child-processes as under Linux, necessitating the usage of a cluster-scheme, which leads to difficulties in actually using functions and variables defined elsewhere, especially in other packages.
* On *Linux*-machines there is `fork()` for parallel processes, which inherit all data obliviating the need for explicit exportation. But GUI-systems (commonly found on workstations) prohibit the usage of these, since they interfere with their inner workings (leading to frequent but intractabel errors)

In the end, those approaches didn lead to noticable gains, mostly the opposite.

**Idea for quantiles:** in order to reduce the complexity-increase associated with the number of observations, one can use quantiles. This means basic pooling of observations into classes which then constitute the elements of further computation. This allows for a flexible reduction of the data-set to manageable sizes. 

since most other algorithms base on cart-greedy, we implemented this there, leading to great reductions in computational cost while maintaining accuracy as needed.

`cart_greedy` was extended with the following parameters:

* `q_threshold`: minimal amount of data points to use quantiles.
* `q_pct`: amount of probabilities for `quantile()`, in pct. of the data set size.
* `type`: `quantile(..., type)`, type of quantile algorithm

Using the `quantile()`-function needs heeding of the fact, that it is only applicable to 1-dimensional vectors. For higher dimensions, one can use for example `sapply`. 



