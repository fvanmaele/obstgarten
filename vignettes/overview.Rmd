---
title: "Obstgarten Overview"
author: "Obstgarten team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Obstgarten Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

Estimation algorithms using trees are a popular class of simple regression and classification estimators, due to their simple tractable structure and comparatively fast evaluation.
This package aims to provide the most popular of these estimators in an easy-to-use implementation.

The given functions are sufficiently fast and versatile to enable the user to explore their behaviour and to experience the effect of their respective hyperparameters. A multitude of visualization methods and an easy to use shiny app leave the user with little to ask for in their quest of evaluating tree-based regression and classification estimators.

*Note*: This package uses the `bbplot` package for the simulation study. It is only available on GitHub: https://github.com/bbc/bbplot

```{r setup}
# library(obstgarten)
suppressMessages(devtools::load_all())
suppressMessages(library(obstgarten))
```

### Trees

Trees are implemented in this package using the following classes:
- `Gabel`, an `R6` class representing a recursive tree data structure with pointers to children and parent nodes. Necessary attributes for use wit CART algorithms are included;
- `Baum`, an `R6` class saving a list of `Gabel` nodes for easy introspection and validation.

As these classes use `R6`, data is stored in environments and reference semantics are used. Trees are created with `Baum$new()` and built up with `Baum$append()`, latter providing a labeling of nodes in *BFS* order.

```{r}
# create new tree and 2 leaves
T1 <- Baum$new()
G1 <- T1$root
G2 <- Gabel$new()
G3 <- Gabel$new()

# populate attributes with random data
G1$s <- runif(1)
G1$j <- sample(10, 1)
G2$y <- runif(1)
G2$y <- runif(1)

# append nodes to root
T1$append(G1, G2, G3)

# view tree as list of lists
lst <- as.list(T1$root)

# view nodes of tree as list
T1$nodes
```

## Algorithms

### Greedy generation of CARTs


### 2.1 greedy.R 

CARTs are generated using the greedy algorithm from [Richter, Definition 6.15]. Sampling of dimensions `d` and amount of leaves in the tree `t` is supported for use with Random Forests.

To implement the CART algorithm, the following functions are used:
- Minimizers `R_hat` and `C_hat` representing minimizers for regression and classification problems, respectively;
- `cart_part` to subdivide a data set according to a given dimension `j` and split point `s`;
- `cart_grid` to compute risk values for points in the data set, represented as a `3D` array;
- `R_min` to compute minimum values in the array returned by `cart_grid`;
- and finally, `cart_greedy` which wraps these functions to build a regression or classification tree.

Example using `sin` data with normally-distributed noise:

```{r}
n <- 150
M <- generate_sin_data(n, sigma=0.2)
dimnames(M) <- list(NULL, c(1, "y"))
T2 <- cart_greedy(M, depth=5, threshold=1)
```

At the random forest the parameter m is selectable here to determine the number of randomly chosen dimensions. In addition, the algorithm always aborts at a random number of created leaves in the tree. This results in a CART that uses only a part of the given knowledge.

```{r}
n <- 150
d <- 4
m <- 2
XY <- generate_mult_data(n, d)[[1]]
cart_greedy(XY, depth=5, random=T, m=m)
```

Richter, Definition 6.15, 6.16, 6.52

#### Mean Square Error

Prediction/MSE of CARTs

**Note**: The `pred` methods allow for computing the MSE of these algorithms and are used for the Shiny app.


### Bagging
 
Generate random forests, with or without sampling. This algorithm uses the `cart_greedy` and `cart_predict` methods from the greedy algorithm described above.

The bootstrap aggregating (Bagging) procedure is an ensemble method from statistical machine learning. It can reduce the model's variance, enhance the accuracy and has a regularization effect. The Bagging procedure adds randomness to the model by averaging (regression) or majority-voting (classification) the prediction for samples `x_i` over the different models that were fitted on the bootstrap samples. The bootstrap samples are created by randomly drawing n samples from the dataset of size `n` with replacement. The number of bootstrap samples B is a hyperparameter of the model.

Random forest works basically like bagging but it uses the modified greedy algorithm with the extra parameter m. This is used to select the number of randomly selected dimensions from the data set. If you don't define m yourself, the heuristically motivated value from the given source is used automatically. With random forest the transitions in the generated CART are usually softer than with bagging, because m generates CARTs with only a part of the knowledge.

See: [Richter, Definition 6.25, 6.26, 6.28, 6.52]

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
x1 <- generate_sin_data(500)
x2 <- generate_sin_data(100)
B <- sample(1:10, 1)
res_bagging <- bagging(B=B, x_train=x1, x_test=x2)

x1 <- generate_sin_2D(500)
x2 <- generate_sin_2D(100)
B <- sample(1:10, 1)
res_random_forest <- bagging(B=B, x_train=x1, x_test=x2, random_forest = T)
```

### Pruning

Cost-complexity pruning of a greedily generated CART, according to a given complexity weight. 
Pruning is a regularization technique which presents a way to decrease computational complexity of empirical estimators as well as simple generalization for overfitting-prone approaches. The cost-complexity parameter lambda controls the amount of regularization.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(simul_plot_pruning())
```
```{r, warning=FALSE, fig.width=7.25, fig.height=5}
load("data/bv_pruning")
suppressMessages(bv_plot(bv_data, plot_title = "Pruning Bias-Variance Tradeoff", bagging = FALSE, pruning=TRUE))
```

## Generation of data 

The package includes methods for generating random data conforming to diverse demands.

```{r}
# multivariate Gaussian
n <- sample(10:20,1)
sigma <- sample(1:3,1)/10
d <- sample(1:5,1)
data <- generate_mult_data(n, d, sd=sigma)

# 2D sine data set
k <- sample(5:15,1)
data <- generate_sin_2D(n, sigma = sigma, k=k)

# 1D sine data set
data <- generate_sin_data(n, sigma = sigma, reg = T, grid = runif(n, min=0, max=1))  
```


## Simulation study

In the simulation study, the four main methods of this package are compared qualitatively as well as quantitatively. The four techniques include decision and regression trees generated by the CART algorithm, pruned CART trees, the CART algorithm augmented by the bootstrap aggregating procedure (bagging) and lastly Random Forests. The methods are further applied to a small real data example, the iris data set. Finally, a short quantitative analysis of the performance optimizing method of using quantiles to find optimal splits instead of single samples only is performed.

See [Simulation study](simul.Rmd)

## Shiny application

Interactive shiny app, sporting all the algorithms above. The parameters for the corresponding algorithms, which can be selected via a menu, are always displayed on the left side for the currently selected algorithm. The corresponding plot on the right hand side is displayed when the "Simulate" button is pressed.

## Performance considerations

### Greedy algorithm

In order to improve the performance of the greedy-algorithm (which is being called repeatedly in bagging, pruning an random forests), there are the following approaches:

* Dynamic programming (*top-down*): memorization of intermediate results
* Dynamic programming (*bottum-up*): build up the tree from below
* Parallelization: split the algorithms at the highest sensible layer into parallel threads
* Quantiles: reduce the amount of data-points for the minimization-problem 

**Dynamic programming** proved to be problematic. The package *memoize* showed in preliminary tests to lead to enormous (up to 700%) losses in performance.

**Idea for quantiles:** in order to reduce the complexity-increase associated with the number of observations, one can use quantiles. This means basic pooling of observations into classes which then constitute the elements of further computation. This allows for a flexible reduction of the data-set to manageable sizes. 

since most other algorithms base on cart-greedy, we implemented this there, leading to great reductions in computational cost while maintaining accuracy as needed. This is particularly useful when generating a greater amount of trees, in particular for bagging and random forests.

`cart_greedy` was extended with the following parameters:

* `quantile`: boolean to enable or disable use of quantiles;
* `q_threshold`: minimal amount of data points to use quantiles;
* `q_pct`: amount of probabilities for `quantile()`, in pct. of the data set size.

Using the `quantile()`-function needs heeding of the fact, that it is only applicable to 1-dimensional vectors. For higher dimensions, one can use for example `sapply`. 

### Random Forests

Algorithms such as bagging and random forests generate a large sequence of CARTs. These trees are independent, and can thus be generated in an independent manner. The package `parallel` provides derivatives of the `apply`-method.  But this works easily only on Linux-systems and has a huge overhead, significantly diminishing the craved results.

* On *Windows*-machines, there are no child-processes as under Linux, necessitating the usage of a cluster-scheme, which leads to difficulties in actually using functions and variables defined elsewhere, especially in other packages.
* On *Linux*-machines there is `fork()` for parallel processes, which inherit all data obliviating the need for explicit exportation. But GUI-systems (commonly found on workstations) prohibit the usage of these, since they interfere with their inner workings (leading to frequent but intractabel errors)

In the end, those approaches did not lead to noticable gains, mostly the opposite.





