---
title: "Obstgarten Overview"
author: "Obstgarten team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Obstgarten Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Contents
Introduction

1. Datenstrukturen
    1. obst.R 
1. Algorithmen
    1. greedy.R 
    1. bagging.R
    1. pruning.R
1. Simulationsstudie
    1. bv.R
    1. data.R
    1. pred.R
    1. simul.R
    1. plot3d.R
1. Shiny
    1. shiny.R
1. Performance considerations

## Introduction

Estimation algorithms using trees are a popular class of simple regression and classification estimators, due to their simple tractable structure and comparatively fast evaluation.
This package aims to provide the most popular of these estimators in an easy-to-use implementation.

The given functions are sufficiently fast and versatile to enable the user to explore their behaviour and to experience the effect of their respective hyperparameters. A multitude of visualization methods and an easy to use shiny app leave the user with little to ask for in their quest of evaluating tree-based regression and classification estimators.

### Trees

Trees are implemented in this package using the following classes:
- `Gabel`, an `R6` class representing a recursive tree data structure with pointers to children and parent nodes. Necessary attributes for use wit CART algorithms are included;
- `Baum`, an `R6` class saving a list of `Gabel` nodes for easy introspection and validation.

As these classes use `R6`, data is stored in environments and reference semantics are used. Trees are created with `Baum$new()` and built up with `Baum$append()`, latter providing a labeling of nodes in *BFS* order.

```{r}
# create new tree and 2 leaves
T1 <- Baum$new()
G1 <- T1$root
G2 <- Gabel$new()
G3 <- Gabel$new()

# populate attributes with random data
G1$s <- runif(1)
G1$j <- sample(10, 1)
G2$y <- runif(1)
G2$y <- runif(1)

# append nodes to root
T1$append(G1, G2, G3)

# view tree as list of lists
lst <- as.list(T1$root)

# view nodes of tree as list
T1$nodes
```

## Algorithms

### Greedy generation of CARTs

CARTs are generated using the greedy algorithm from [Richter, Definition 6.15]. Sampling of dimensions `d` and amount of leaves in the tree `t` is supported for use with Random Forests.

To implement the CART algorithm, the following functions are used:
- Minimizers `R_hat` and `C_hat` representing minimizers for regression and classification problems, respectively;
- `cart_part` to subdivide a data set according to a given dimension `j` and split point `s`;
- `cart_grid` to compute risk values for points in the data set, represented as a `3D` array;
- `R_min` to compute minimum values in the array returned by `cart_grid`;
- and finally, `cart_greedy` which wraps these functions to build a regression or classification tree.

Example using `sin` data with normally-distributed noise:

```{r}

```

#### Mean Square Error

Prediction/MSE of CARTs

```{r}

```


### Bagging
 
Generate random forests, with or without sampling. This algorithm uses the `cart_greedy` and `cart_predict` methods from the greedy algorithm described above.

The bootstrap aggregating (Bagging) procedure is an ensemble method from statistical machine learning. It can reduce the model's variance, enhance the accuracy and has a regularization effect. The Bagging procedure adds randomness to the model by averaging (regression) or majority-voting (classification) the prediction for samples `x_i` over the different models that were fitted on the bootstrap samples. The bootstrap samples are created by randomly drawing n samples from the dataset of size `n` with replacement. The number of bootstrap samples B is a hyperparameter of the model.

See: [Richter, Definition 6.25, 6.26, 6.28]

```{r}

```

### Pruning

Cost-complexity pruning of a greedily generated CART, according to a given complexity weight. 
Pruning is a regularization technique which presents a way to decrease computational complexity of empirical estimators as well as simple generalization for overfitting-prone approaches. The cost-complexity parameter lambda controls the amount of regularization.

```{r, warning=FALSE, fig.width=7.25, fig.height=5}
suppressMessages(simul_plot_pruning())
```
```{r, warning=FALSE, fig.width=7.25, fig.height=5}
load("data/bv_pruning")
suppressMessages(bv_plot(bv_data, plot_title = "Pruning Bias-Variance Tradeoff", bagging = FALSE, pruning=TRUE))
```

## Generation of data 

Generating random data conforming to diverse demands.

```{r}

```


## Plots

```{r}

```


## Simulation study

Here the four main methods of this package are studied experimentally. The four techniques include decision and regression trees generated by the CART algorithm, pruned CART trees, the CART algorithm augmented by the bootstrap aggregating procedure (bagging) and lastly Random Forests. 

After looking at the methods themselves and some of their hyperparameters in the first section, the methods are compared qualitatively as well as quantitatively in the second section. Most of the results present some form of regression problem. Note that all the results very well are reproducible in the classification case. See the shiny-app for reference. In the third section the methods are applied to a small real data example: The Iris Dataset. The simulation study is closed ba a short quantitative analysis of the performance optimizing method of using quantiles in the process of finding optimal splits instead of single samples only.

## Shiny application

Interactive shiny app, sporting all the algorithms above.

## Performance considerations

### Greedy algorithm

In order to improve the performance of the greedy-algorithm (which is being called repeatedly in bagging, pruning an random forests), there are the following approaches:

* Dynamic programming (*top-down*): memorization of intermediate results
* Dynamic programming (*bottum-up*): build up the tree from below
* Parallelization: split the algorithms at the highest sensible layer into parallel threads
* Quantiles: reduce the amount of data-points for the minimization-problem 

**Dynamic programming** proved to be problematic. The package *memoize* showed in preliminary tests to lead to enormous (up to 700%) losses in performance.

**Idea for quantiles:** in order to reduce the complexity-increase associated with the number of observations, one can use quantiles. This means basic pooling of observations into classes which then constitute the elements of further computation. This allows for a flexible reduction of the data-set to manageable sizes. 

since most other algorithms base on cart-greedy, we implemented this there, leading to great reductions in computational cost while maintaining accuracy as needed.

`cart_greedy` was extended with the following parameters:

* `quantile`: boolean to enable or disable use of quantiles;
* `q_threshold`: minimal amount of data points to use quantiles;
* `q_pct`: amount of probabilities for `quantile()`, in pct. of the data set size.

Using the `quantile()`-function needs heeding of the fact, that it is only applicable to 1-dimensional vectors. For higher dimensions, one can use for example `sapply`. 

```{r}

```

### Random Forests

Algorithms such as bagging and random forests generate a large sequence of CARTs. These trees are independent, and can thus be generated in an independent manner. The package `parallel` provides derivatives of the `apply`-method.  But this works easily only on Linux-systems and has a huge overhead, significantly diminishing the craved results.

* On *Windows*-machines, there are no child-processes as under Linux, necessitating the usage of a cluster-scheme, which leads to difficulties in actually using functions and variables defined elsewhere, especially in other packages.
* On *Linux*-machines there is `fork()` for parallel processes, which inherit all data obliviating the need for explicit exportation. But GUI-systems (commonly found on workstations) prohibit the usage of these, since they interfere with their inner workings (leading to frequent but intractabel errors)

In the end, those approaches did not lead to noticable gains, mostly the opposite.





